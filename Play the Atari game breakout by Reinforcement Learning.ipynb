{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3e0110c",
   "metadata": {},
   "source": [
    "Important: The following materials come from the book \"Hands-on Machine Learning with Scikit-Learn, Keras and Tensorflow 2nd Edition\" by Aurelien Geron. This notebook is for the readers to get familiar with Reinforcement Learning using Keras and Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4d0b3c",
   "metadata": {},
   "source": [
    "(A) Creating the \"Breakout\" environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "681bcf24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf_agents.environments.gym_wrapper.GymWrapper at 0x2998a0da1f0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tf_agents.environments import suite_gym\n",
    "env = suite_gym.load(\"Breakout-v4\")\n",
    "env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c417261f",
   "metadata": {},
   "source": [
    "(B) Getting familiar with the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c87db411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OrderEnforcing<PassiveEnvChecker<AtariEnv<Breakout-v4>>>>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4d0a852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOOP', 'FIRE', 'RIGHT', 'LEFT']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.gym.get_action_meanings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc62f3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(env.current_time_step())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90d4c61",
   "metadata": {},
   "source": [
    "(C) Take a look at the environment specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f52706e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundedArraySpec(shape=(210, 160, 3), dtype=dtype('uint8'), name='observation', minimum=0, maximum=255)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "028f3658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundedArraySpec(shape=(), dtype=dtype('int64'), name='action', minimum=0, maximum=3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24fb7bdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(\n",
       "{'discount': BoundedArraySpec(shape=(), dtype=dtype('float32'), name='discount', minimum=0.0, maximum=1.0),\n",
       " 'observation': BoundedArraySpec(shape=(210, 160, 3), dtype=dtype('uint8'), name='observation', minimum=0, maximum=255),\n",
       " 'reward': ArraySpec(shape=(), dtype=dtype('float32'), name='reward'),\n",
       " 'step_type': ArraySpec(shape=(), dtype=dtype('int32'), name='step_type')})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.time_step_spec()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17563fe",
   "metadata": {},
   "source": [
    "(D) Creating a wrapped Breakout environment with preprocessing and Tensorflow support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b978625f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.environments import suite_atari\n",
    "from tf_agents.environments.atari_preprocessing import AtariPreprocessing\n",
    "from tf_agents.environments.atari_wrappers import FrameStack4\n",
    "\n",
    "max_episode_steps = 27000\n",
    "environment_name = \"BreakoutNoFrameskip-v4\"\n",
    "\n",
    "env = suite_atari.load(\n",
    "        environment_name,\n",
    "        max_episode_steps = max_episode_steps,\n",
    "        gym_env_wrappers=[AtariPreprocessing, FrameStack4])\n",
    "\n",
    "from tf_agents.environments.tf_py_environment import TFPyEnvironment\n",
    "\n",
    "tf_env = TFPyEnvironment(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e9bf33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf_agents.environments.tf_py_environment.TFPyEnvironment object at 0x000002998A0DAB50>\n"
     ]
    }
   ],
   "source": [
    "print(tf_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e669399e",
   "metadata": {},
   "source": [
    "(E) Creating the Deep Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e81df1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tf_agents.networks.q_network import QNetwork\n",
    "\n",
    "preprocessing_layer = keras.layers.Lambda(\n",
    "                          lambda obs: tf.cast(obs, np.float32) / 255.)\n",
    "conv_layer_params=[(32, (8, 8), 4), (64, (4, 4), 2), (64, (3, 3), 1)]\n",
    "fc_layer_params=[512]\n",
    "\n",
    "q_net = QNetwork(\n",
    "    tf_env.observation_spec(),\n",
    "    tf_env.action_spec(),\n",
    "    preprocessing_layers=preprocessing_layer,\n",
    "    conv_layer_params=conv_layer_params,\n",
    "    fc_layer_params=fc_layer_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b04bbf",
   "metadata": {},
   "source": [
    "(F) Creating the DQN agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "21562c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tf_agents.agents.dqn.dqn_agent import DqnAgent\n",
    "\n",
    "train_step = tf.Variable(0)\n",
    "update_period = 4 # run a training step every 4 collect steps\n",
    "optimizer = keras.optimizers.RMSprop(learning_rate=2.5e-4, rho=0.95, momentum=0.0,\n",
    "                                     epsilon=0.00001, centered=True)\n",
    "epsilon_fn = keras.optimizers.schedules.PolynomialDecay(\n",
    "    initial_learning_rate=1.0, # initial ε\n",
    "    decay_steps=250000 // update_period, # <=> 1,000,000 ALE frames\n",
    "    end_learning_rate=0.01) # final ε\n",
    "agent = DqnAgent(tf_env.time_step_spec(),\n",
    "                 tf_env.action_spec(),\n",
    "                 q_network=q_net,\n",
    "                 optimizer=optimizer,\n",
    "                 target_update_period=2000, # <=> 32,000 ALE frames\n",
    "                 td_errors_loss_fn=keras.losses.Huber(reduction=\"none\"),\n",
    "                 gamma=0.99, # discount factor\n",
    "                 train_step_counter=train_step,\n",
    "                 epsilon_greedy=lambda: epsilon_fn(train_step))\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f70167",
   "metadata": {},
   "source": [
    "(G) Creating the Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e586c45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=tf_env.batch_size,\n",
    "    max_length=100000) # reduce if OOM error\n",
    "\n",
    "replay_buffer_observer = replay_buffer.add_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa53b7f",
   "metadata": {},
   "source": [
    "(H) Creating a custom observer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d38e2d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShowProgress:\n",
    "    def __init__(self, total):\n",
    "        self.counter = 0\n",
    "        self.total = total\n",
    "    def __call__(self, trajectory):\n",
    "        if not trajectory.is_boundary():\n",
    "            self.counter += 1\n",
    "        if self.counter % 100 == 0:\n",
    "            print(\"\\r{}/{}\".format(self.counter, self.total), end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401a7eb6",
   "metadata": {},
   "source": [
    "(I) Adding some training metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0f39a67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.metrics import tf_metrics\n",
    "\n",
    "train_metrics = [\n",
    "    tf_metrics.NumberOfEpisodes(),\n",
    "    tf_metrics.EnvironmentSteps(),\n",
    "    tf_metrics.AverageReturnMetric(),\n",
    "    tf_metrics.AverageEpisodeLengthMetric(),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a5b0293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int64, numpy=0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_metrics[0].result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9584bd0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
